{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=pd.read_csv(\"C:\\\\Users\\\\PRASHANT\\\\Desktop\\\\tensorflow udemy course\\\\TF_2_Notebooks_and_Data\\\\DATA\\\\iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=iris.drop('species',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0             5.1          3.5           1.4          0.2\n",
       "1             4.9          3.0           1.4          0.2\n",
       "2             4.7          3.2           1.3          0.2\n",
       "3             4.6          3.1           1.5          0.2\n",
       "4             5.0          3.6           1.4          0.2\n",
       "..            ...          ...           ...          ...\n",
       "145           6.7          3.0           5.2          2.3\n",
       "146           6.3          2.5           5.0          1.9\n",
       "147           6.5          3.0           5.2          2.0\n",
       "148           6.2          3.4           5.4          2.3\n",
       "149           5.9          3.0           5.1          1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         setosa\n",
       "1         setosa\n",
       "2         setosa\n",
       "3         setosa\n",
       "4         setosa\n",
       "         ...    \n",
       "145    virginica\n",
       "146    virginica\n",
       "147    virginica\n",
       "148    virginica\n",
       "149    virginica\n",
       "Name: species, Length: 150, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X is 2d matrix and y is one dimensional matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here there are three classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " encoder=LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y  # y is now one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y is 1 or 0 based on species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 0 0 in one hot encoded means flower is setosa and 0 1 0 means flower is versicolor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "#sinc there are only 4 feature and we are not worry about batches hence input shape is [4,]\n",
    "model.add(Dense(units=3,activation='softmax'))# output belong to three class\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])#in metrics we specify that we will keep\n",
    "#trac of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping=EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 4s 36ms/sample - loss: 1.1839 - accuracy: 0.3500 - val_loss: 1.2260 - val_accuracy: 0.2667\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 1ms/sample - loss: 1.1805 - accuracy: 0.3500 - val_loss: 1.2229 - val_accuracy: 0.2667\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 796us/sample - loss: 1.1778 - accuracy: 0.3500 - val_loss: 1.2194 - val_accuracy: 0.2667\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 445us/sample - loss: 1.1748 - accuracy: 0.3500 - val_loss: 1.2159 - val_accuracy: 0.2667\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 461us/sample - loss: 1.1718 - accuracy: 0.3500 - val_loss: 1.2128 - val_accuracy: 0.2667\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 605us/sample - loss: 1.1691 - accuracy: 0.3500 - val_loss: 1.2097 - val_accuracy: 0.2667\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 421us/sample - loss: 1.1664 - accuracy: 0.3500 - val_loss: 1.2067 - val_accuracy: 0.2667\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 507us/sample - loss: 1.1638 - accuracy: 0.3500 - val_loss: 1.2037 - val_accuracy: 0.2667\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 521us/sample - loss: 1.1613 - accuracy: 0.3500 - val_loss: 1.2007 - val_accuracy: 0.2667\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 901us/sample - loss: 1.1586 - accuracy: 0.3500 - val_loss: 1.1978 - val_accuracy: 0.2667\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 839us/sample - loss: 1.1562 - accuracy: 0.3500 - val_loss: 1.1951 - val_accuracy: 0.2667\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 502us/sample - loss: 1.1535 - accuracy: 0.3500 - val_loss: 1.1925 - val_accuracy: 0.2667\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 425us/sample - loss: 1.1514 - accuracy: 0.3500 - val_loss: 1.1898 - val_accuracy: 0.2667\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 529us/sample - loss: 1.1489 - accuracy: 0.3500 - val_loss: 1.1872 - val_accuracy: 0.2667\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 455us/sample - loss: 1.1467 - accuracy: 0.3500 - val_loss: 1.1846 - val_accuracy: 0.2667\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 455us/sample - loss: 1.1443 - accuracy: 0.3500 - val_loss: 1.1821 - val_accuracy: 0.2667\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 486us/sample - loss: 1.1419 - accuracy: 0.3500 - val_loss: 1.1799 - val_accuracy: 0.2667\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 481us/sample - loss: 1.1398 - accuracy: 0.3500 - val_loss: 1.1776 - val_accuracy: 0.2667\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 1.1375 - accuracy: 0.3500 - val_loss: 1.1755 - val_accuracy: 0.2667\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 660us/sample - loss: 1.1352 - accuracy: 0.3500 - val_loss: 1.1733 - val_accuracy: 0.2667\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 453us/sample - loss: 1.1330 - accuracy: 0.3500 - val_loss: 1.1713 - val_accuracy: 0.2667\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 1.1308 - accuracy: 0.3500 - val_loss: 1.1692 - val_accuracy: 0.2667\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 683us/sample - loss: 1.1285 - accuracy: 0.3500 - val_loss: 1.1672 - val_accuracy: 0.2667\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 595us/sample - loss: 1.1264 - accuracy: 0.3500 - val_loss: 1.1653 - val_accuracy: 0.2667\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 543us/sample - loss: 1.1241 - accuracy: 0.3500 - val_loss: 1.1633 - val_accuracy: 0.3000\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 452us/sample - loss: 1.1219 - accuracy: 0.3500 - val_loss: 1.1612 - val_accuracy: 0.3000\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 524us/sample - loss: 1.1197 - accuracy: 0.3500 - val_loss: 1.1591 - val_accuracy: 0.3000\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 485us/sample - loss: 1.1176 - accuracy: 0.3500 - val_loss: 1.1570 - val_accuracy: 0.3000\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 537us/sample - loss: 1.1153 - accuracy: 0.3500 - val_loss: 1.1549 - val_accuracy: 0.3000\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 547us/sample - loss: 1.1131 - accuracy: 0.3500 - val_loss: 1.1528 - val_accuracy: 0.3000\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 734us/sample - loss: 1.1108 - accuracy: 0.3500 - val_loss: 1.1507 - val_accuracy: 0.2667\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 498us/sample - loss: 1.1086 - accuracy: 0.3500 - val_loss: 1.1487 - val_accuracy: 0.2667\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 500us/sample - loss: 1.1063 - accuracy: 0.3500 - val_loss: 1.1466 - val_accuracy: 0.2667\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 507us/sample - loss: 1.1040 - accuracy: 0.3500 - val_loss: 1.1446 - val_accuracy: 0.2667\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 509us/sample - loss: 1.1017 - accuracy: 0.3500 - val_loss: 1.1426 - val_accuracy: 0.2667\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 512us/sample - loss: 1.0995 - accuracy: 0.3500 - val_loss: 1.1406 - val_accuracy: 0.2667\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 549us/sample - loss: 1.0972 - accuracy: 0.3500 - val_loss: 1.1386 - val_accuracy: 0.2667\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 485us/sample - loss: 1.0948 - accuracy: 0.3500 - val_loss: 1.1365 - val_accuracy: 0.2667\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 398us/sample - loss: 1.0926 - accuracy: 0.3500 - val_loss: 1.1346 - val_accuracy: 0.2667\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 446us/sample - loss: 1.0903 - accuracy: 0.3500 - val_loss: 1.1327 - val_accuracy: 0.2667\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 459us/sample - loss: 1.0879 - accuracy: 0.3500 - val_loss: 1.1306 - val_accuracy: 0.2667\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 460us/sample - loss: 1.0858 - accuracy: 0.3500 - val_loss: 1.1287 - val_accuracy: 0.2667\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 468us/sample - loss: 1.0834 - accuracy: 0.3500 - val_loss: 1.1265 - val_accuracy: 0.2667\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 471us/sample - loss: 1.0810 - accuracy: 0.3500 - val_loss: 1.1245 - val_accuracy: 0.2667\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 415us/sample - loss: 1.0788 - accuracy: 0.3500 - val_loss: 1.1226 - val_accuracy: 0.2667\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 440us/sample - loss: 1.0764 - accuracy: 0.3500 - val_loss: 1.1206 - val_accuracy: 0.2667\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 484us/sample - loss: 1.0741 - accuracy: 0.3500 - val_loss: 1.1186 - val_accuracy: 0.2667\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 857us/sample - loss: 1.0718 - accuracy: 0.3500 - val_loss: 1.1166 - val_accuracy: 0.2667\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 611us/sample - loss: 1.0694 - accuracy: 0.3500 - val_loss: 1.1143 - val_accuracy: 0.2667\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 945us/sample - loss: 1.0670 - accuracy: 0.3583 - val_loss: 1.1120 - val_accuracy: 0.2667\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 614us/sample - loss: 1.0644 - accuracy: 0.3583 - val_loss: 1.1096 - val_accuracy: 0.2667\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 813us/sample - loss: 1.0616 - accuracy: 0.3583 - val_loss: 1.1066 - val_accuracy: 0.2667\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 1.0586 - accuracy: 0.3583 - val_loss: 1.1036 - val_accuracy: 0.2667\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 470us/sample - loss: 1.0556 - accuracy: 0.3583 - val_loss: 1.1001 - val_accuracy: 0.2667\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 504us/sample - loss: 1.0526 - accuracy: 0.3583 - val_loss: 1.0964 - val_accuracy: 0.2667\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 456us/sample - loss: 1.0491 - accuracy: 0.3833 - val_loss: 1.0925 - val_accuracy: 0.3000\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 816us/sample - loss: 1.0455 - accuracy: 0.3833 - val_loss: 1.0886 - val_accuracy: 0.3333\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 731us/sample - loss: 1.0422 - accuracy: 0.3917 - val_loss: 1.0848 - val_accuracy: 0.4000\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 556us/sample - loss: 1.0394 - accuracy: 0.3917 - val_loss: 1.0815 - val_accuracy: 0.4000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 717us/sample - loss: 1.0366 - accuracy: 0.4083 - val_loss: 1.0787 - val_accuracy: 0.4000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 513us/sample - loss: 1.0339 - accuracy: 0.4083 - val_loss: 1.0757 - val_accuracy: 0.4000\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 738us/sample - loss: 1.0312 - accuracy: 0.4083 - val_loss: 1.0732 - val_accuracy: 0.4000\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 471us/sample - loss: 1.0285 - accuracy: 0.4167 - val_loss: 1.0704 - val_accuracy: 0.4000\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 513us/sample - loss: 1.0258 - accuracy: 0.4083 - val_loss: 1.0678 - val_accuracy: 0.4000\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 585us/sample - loss: 1.0230 - accuracy: 0.4083 - val_loss: 1.0651 - val_accuracy: 0.4000\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 594us/sample - loss: 1.0202 - accuracy: 0.4083 - val_loss: 1.0624 - val_accuracy: 0.4000\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 886us/sample - loss: 1.0173 - accuracy: 0.4083 - val_loss: 1.0597 - val_accuracy: 0.4000\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 816us/sample - loss: 1.0145 - accuracy: 0.4083 - val_loss: 1.0569 - val_accuracy: 0.4000\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 516us/sample - loss: 1.0118 - accuracy: 0.4083 - val_loss: 1.0544 - val_accuracy: 0.4000\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 1.0086 - accuracy: 0.4083 - val_loss: 1.0515 - val_accuracy: 0.4333\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 947us/sample - loss: 1.0058 - accuracy: 0.4083 - val_loss: 1.0482 - val_accuracy: 0.4333\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 850us/sample - loss: 1.0028 - accuracy: 0.4083 - val_loss: 1.0455 - val_accuracy: 0.4333\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 587us/sample - loss: 0.9997 - accuracy: 0.4083 - val_loss: 1.0425 - val_accuracy: 0.4333\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 888us/sample - loss: 0.9967 - accuracy: 0.4250 - val_loss: 1.0395 - val_accuracy: 0.4333\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 889us/sample - loss: 0.9937 - accuracy: 0.4333 - val_loss: 1.0367 - val_accuracy: 0.4333\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 781us/sample - loss: 0.9907 - accuracy: 0.4333 - val_loss: 1.0340 - val_accuracy: 0.4333\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 708us/sample - loss: 0.9876 - accuracy: 0.4417 - val_loss: 1.0310 - val_accuracy: 0.4333\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 726us/sample - loss: 0.9845 - accuracy: 0.4583 - val_loss: 1.0282 - val_accuracy: 0.4333\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 531us/sample - loss: 0.9817 - accuracy: 0.4583 - val_loss: 1.0257 - val_accuracy: 0.4333\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 837us/sample - loss: 0.9783 - accuracy: 0.4667 - val_loss: 1.0225 - val_accuracy: 0.4333\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 495us/sample - loss: 0.9752 - accuracy: 0.4667 - val_loss: 1.0194 - val_accuracy: 0.4333\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 758us/sample - loss: 0.9720 - accuracy: 0.4667 - val_loss: 1.0166 - val_accuracy: 0.4333\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 502us/sample - loss: 0.9687 - accuracy: 0.4833 - val_loss: 1.0135 - val_accuracy: 0.4333\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.9655 - accuracy: 0.4833 - val_loss: 1.0103 - val_accuracy: 0.4333\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 836us/sample - loss: 0.9623 - accuracy: 0.4833 - val_loss: 1.0070 - val_accuracy: 0.4333\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 572us/sample - loss: 0.9590 - accuracy: 0.4833 - val_loss: 1.0039 - val_accuracy: 0.4333\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 730us/sample - loss: 0.9557 - accuracy: 0.4833 - val_loss: 1.0007 - val_accuracy: 0.4333\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 544us/sample - loss: 0.9523 - accuracy: 0.5167 - val_loss: 0.9977 - val_accuracy: 0.4333\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 518us/sample - loss: 0.9489 - accuracy: 0.5250 - val_loss: 0.9944 - val_accuracy: 0.4333\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 461us/sample - loss: 0.9455 - accuracy: 0.5333 - val_loss: 0.9911 - val_accuracy: 0.4333\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 812us/sample - loss: 0.9421 - accuracy: 0.5500 - val_loss: 0.9876 - val_accuracy: 0.4333\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.9386 - accuracy: 0.5583 - val_loss: 0.9844 - val_accuracy: 0.4667\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 461us/sample - loss: 0.9351 - accuracy: 0.5583 - val_loss: 0.9814 - val_accuracy: 0.4667\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.9317 - accuracy: 0.5667 - val_loss: 0.9782 - val_accuracy: 0.4667\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - 0s 412us/sample - loss: 0.9282 - accuracy: 0.5667 - val_loss: 0.9751 - val_accuracy: 0.4667\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.9249 - accuracy: 0.5833 - val_loss: 0.9724 - val_accuracy: 0.4667\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 503us/sample - loss: 0.9212 - accuracy: 0.6083 - val_loss: 0.9692 - val_accuracy: 0.4667\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 677us/sample - loss: 0.9177 - accuracy: 0.6167 - val_loss: 0.9659 - val_accuracy: 0.5000\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 530us/sample - loss: 0.9141 - accuracy: 0.6333 - val_loss: 0.9625 - val_accuracy: 0.5000\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 407us/sample - loss: 0.9106 - accuracy: 0.6417 - val_loss: 0.9590 - val_accuracy: 0.5000\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 692us/sample - loss: 0.9072 - accuracy: 0.6583 - val_loss: 0.9558 - val_accuracy: 0.5000\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 530us/sample - loss: 0.9035 - accuracy: 0.6750 - val_loss: 0.9520 - val_accuracy: 0.5333\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 548us/sample - loss: 0.9002 - accuracy: 0.6750 - val_loss: 0.9480 - val_accuracy: 0.5667\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 684us/sample - loss: 0.8963 - accuracy: 0.6750 - val_loss: 0.9446 - val_accuracy: 0.5667\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 494us/sample - loss: 0.8929 - accuracy: 0.7000 - val_loss: 0.9415 - val_accuracy: 0.6000\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 0.8892 - accuracy: 0.7083 - val_loss: 0.9381 - val_accuracy: 0.6000\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 472us/sample - loss: 0.8858 - accuracy: 0.7083 - val_loss: 0.9349 - val_accuracy: 0.6000\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 467us/sample - loss: 0.8822 - accuracy: 0.7167 - val_loss: 0.9313 - val_accuracy: 0.6000\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 454us/sample - loss: 0.8786 - accuracy: 0.7250 - val_loss: 0.9279 - val_accuracy: 0.6000\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 565us/sample - loss: 0.8750 - accuracy: 0.7250 - val_loss: 0.9245 - val_accuracy: 0.6333\n",
      "Epoch 111/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 836us/sample - loss: 0.8715 - accuracy: 0.7333 - val_loss: 0.9210 - val_accuracy: 0.6333\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 585us/sample - loss: 0.8679 - accuracy: 0.7417 - val_loss: 0.9175 - val_accuracy: 0.6667\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 584us/sample - loss: 0.8645 - accuracy: 0.7417 - val_loss: 0.9140 - val_accuracy: 0.7000\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 849us/sample - loss: 0.8609 - accuracy: 0.7417 - val_loss: 0.9105 - val_accuracy: 0.7000\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 865us/sample - loss: 0.8574 - accuracy: 0.7417 - val_loss: 0.9070 - val_accuracy: 0.7000\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 762us/sample - loss: 0.8539 - accuracy: 0.7500 - val_loss: 0.9042 - val_accuracy: 0.7000\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.8504 - accuracy: 0.7583 - val_loss: 0.9010 - val_accuracy: 0.7000\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 556us/sample - loss: 0.8469 - accuracy: 0.7583 - val_loss: 0.8974 - val_accuracy: 0.7000\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 786us/sample - loss: 0.8434 - accuracy: 0.7583 - val_loss: 0.8940 - val_accuracy: 0.7000\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 485us/sample - loss: 0.8399 - accuracy: 0.7583 - val_loss: 0.8905 - val_accuracy: 0.7000\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 483us/sample - loss: 0.8365 - accuracy: 0.7583 - val_loss: 0.8868 - val_accuracy: 0.7000\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 471us/sample - loss: 0.8330 - accuracy: 0.7583 - val_loss: 0.8834 - val_accuracy: 0.7000\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 484us/sample - loss: 0.8297 - accuracy: 0.7583 - val_loss: 0.8800 - val_accuracy: 0.7333\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 472us/sample - loss: 0.8263 - accuracy: 0.7583 - val_loss: 0.8769 - val_accuracy: 0.7333\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 448us/sample - loss: 0.8229 - accuracy: 0.7583 - val_loss: 0.8736 - val_accuracy: 0.7333\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 442us/sample - loss: 0.8195 - accuracy: 0.7667 - val_loss: 0.8707 - val_accuracy: 0.7333\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 527us/sample - loss: 0.8162 - accuracy: 0.7667 - val_loss: 0.8677 - val_accuracy: 0.7333\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 494us/sample - loss: 0.8128 - accuracy: 0.7667 - val_loss: 0.8646 - val_accuracy: 0.7333\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 869us/sample - loss: 0.8095 - accuracy: 0.7667 - val_loss: 0.8614 - val_accuracy: 0.7333\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 867us/sample - loss: 0.8063 - accuracy: 0.7667 - val_loss: 0.8579 - val_accuracy: 0.7333\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 570us/sample - loss: 0.8032 - accuracy: 0.7667 - val_loss: 0.8544 - val_accuracy: 0.7333\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 872us/sample - loss: 0.7998 - accuracy: 0.7667 - val_loss: 0.8516 - val_accuracy: 0.7333\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 788us/sample - loss: 0.7965 - accuracy: 0.7750 - val_loss: 0.8487 - val_accuracy: 0.7333\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 611us/sample - loss: 0.7934 - accuracy: 0.7750 - val_loss: 0.8458 - val_accuracy: 0.7333\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 810us/sample - loss: 0.7902 - accuracy: 0.7750 - val_loss: 0.8426 - val_accuracy: 0.7333\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 519us/sample - loss: 0.7870 - accuracy: 0.7750 - val_loss: 0.8392 - val_accuracy: 0.7333\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 504us/sample - loss: 0.7839 - accuracy: 0.7750 - val_loss: 0.8360 - val_accuracy: 0.7333\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 923us/sample - loss: 0.7807 - accuracy: 0.7833 - val_loss: 0.8330 - val_accuracy: 0.7333\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 596us/sample - loss: 0.7777 - accuracy: 0.7833 - val_loss: 0.8297 - val_accuracy: 0.7333\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.7746 - accuracy: 0.7833 - val_loss: 0.8266 - val_accuracy: 0.7333\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 557us/sample - loss: 0.7715 - accuracy: 0.7833 - val_loss: 0.8236 - val_accuracy: 0.7333\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 621us/sample - loss: 0.7684 - accuracy: 0.7833 - val_loss: 0.8208 - val_accuracy: 0.7333\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 553us/sample - loss: 0.7654 - accuracy: 0.7917 - val_loss: 0.8179 - val_accuracy: 0.7333\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 555us/sample - loss: 0.7624 - accuracy: 0.7917 - val_loss: 0.8147 - val_accuracy: 0.7333\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 901us/sample - loss: 0.7595 - accuracy: 0.7917 - val_loss: 0.8119 - val_accuracy: 0.7333\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 579us/sample - loss: 0.7565 - accuracy: 0.7917 - val_loss: 0.8090 - val_accuracy: 0.7667\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 892us/sample - loss: 0.7535 - accuracy: 0.7917 - val_loss: 0.8062 - val_accuracy: 0.7667\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 829us/sample - loss: 0.7508 - accuracy: 0.7917 - val_loss: 0.8037 - val_accuracy: 0.7667\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 503us/sample - loss: 0.7477 - accuracy: 0.7917 - val_loss: 0.8007 - val_accuracy: 0.7667\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 865us/sample - loss: 0.7448 - accuracy: 0.7917 - val_loss: 0.7977 - val_accuracy: 0.7667\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 879us/sample - loss: 0.7420 - accuracy: 0.7917 - val_loss: 0.7947 - val_accuracy: 0.7667\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 832us/sample - loss: 0.7391 - accuracy: 0.7917 - val_loss: 0.7916 - val_accuracy: 0.7667\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 763us/sample - loss: 0.7366 - accuracy: 0.7917 - val_loss: 0.7882 - val_accuracy: 0.7667\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 481us/sample - loss: 0.7337 - accuracy: 0.7917 - val_loss: 0.7854 - val_accuracy: 0.7667\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 459us/sample - loss: 0.7307 - accuracy: 0.7917 - val_loss: 0.7825 - val_accuracy: 0.7667\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 921us/sample - loss: 0.7280 - accuracy: 0.7917 - val_loss: 0.7796 - val_accuracy: 0.7667\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 921us/sample - loss: 0.7252 - accuracy: 0.8000 - val_loss: 0.7766 - val_accuracy: 0.7667\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 742us/sample - loss: 0.7225 - accuracy: 0.8000 - val_loss: 0.7739 - val_accuracy: 0.7667\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 608us/sample - loss: 0.7199 - accuracy: 0.8083 - val_loss: 0.7713 - val_accuracy: 0.7667\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.7171 - accuracy: 0.8083 - val_loss: 0.7686 - val_accuracy: 0.8000\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 503us/sample - loss: 0.7145 - accuracy: 0.8083 - val_loss: 0.7658 - val_accuracy: 0.8000\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 463us/sample - loss: 0.7119 - accuracy: 0.8083 - val_loss: 0.7634 - val_accuracy: 0.8000\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 604us/sample - loss: 0.7093 - accuracy: 0.8083 - val_loss: 0.7611 - val_accuracy: 0.8000\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 547us/sample - loss: 0.7067 - accuracy: 0.8083 - val_loss: 0.7587 - val_accuracy: 0.8000\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 464us/sample - loss: 0.7042 - accuracy: 0.8083 - val_loss: 0.7562 - val_accuracy: 0.8000\n",
      "Epoch 166/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 433us/sample - loss: 0.7017 - accuracy: 0.8083 - val_loss: 0.7539 - val_accuracy: 0.8000\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 680us/sample - loss: 0.6993 - accuracy: 0.8000 - val_loss: 0.7514 - val_accuracy: 0.8000\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 459us/sample - loss: 0.6969 - accuracy: 0.8000 - val_loss: 0.7494 - val_accuracy: 0.7667\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 785us/sample - loss: 0.6944 - accuracy: 0.8000 - val_loss: 0.7471 - val_accuracy: 0.7667\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 793us/sample - loss: 0.6920 - accuracy: 0.7917 - val_loss: 0.7450 - val_accuracy: 0.7667\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 554us/sample - loss: 0.6895 - accuracy: 0.7917 - val_loss: 0.7425 - val_accuracy: 0.7667\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.6871 - accuracy: 0.8000 - val_loss: 0.7401 - val_accuracy: 0.7667\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 727us/sample - loss: 0.6848 - accuracy: 0.8000 - val_loss: 0.7375 - val_accuracy: 0.8000\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 538us/sample - loss: 0.6825 - accuracy: 0.8000 - val_loss: 0.7351 - val_accuracy: 0.8000\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 542us/sample - loss: 0.6801 - accuracy: 0.8083 - val_loss: 0.7326 - val_accuracy: 0.8000\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 799us/sample - loss: 0.6778 - accuracy: 0.8083 - val_loss: 0.7300 - val_accuracy: 0.8000\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.78 - 0s 589us/sample - loss: 0.6756 - accuracy: 0.8083 - val_loss: 0.7274 - val_accuracy: 0.8000\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 866us/sample - loss: 0.6732 - accuracy: 0.8083 - val_loss: 0.7251 - val_accuracy: 0.8000\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 498us/sample - loss: 0.6709 - accuracy: 0.8083 - val_loss: 0.7229 - val_accuracy: 0.8000\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 792us/sample - loss: 0.6687 - accuracy: 0.8083 - val_loss: 0.7206 - val_accuracy: 0.8000\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 498us/sample - loss: 0.6664 - accuracy: 0.8083 - val_loss: 0.7186 - val_accuracy: 0.8000\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 503us/sample - loss: 0.6645 - accuracy: 0.8083 - val_loss: 0.7168 - val_accuracy: 0.8000\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.6621 - accuracy: 0.8083 - val_loss: 0.7144 - val_accuracy: 0.8000\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 447us/sample - loss: 0.6599 - accuracy: 0.8083 - val_loss: 0.7121 - val_accuracy: 0.8000\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 520us/sample - loss: 0.6578 - accuracy: 0.8083 - val_loss: 0.7097 - val_accuracy: 0.8000\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 468us/sample - loss: 0.6556 - accuracy: 0.8083 - val_loss: 0.7076 - val_accuracy: 0.8000\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 482us/sample - loss: 0.6536 - accuracy: 0.8083 - val_loss: 0.7055 - val_accuracy: 0.8000\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 428us/sample - loss: 0.6514 - accuracy: 0.8083 - val_loss: 0.7033 - val_accuracy: 0.8000\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 502us/sample - loss: 0.6494 - accuracy: 0.8083 - val_loss: 0.7012 - val_accuracy: 0.8000\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 495us/sample - loss: 0.6475 - accuracy: 0.8167 - val_loss: 0.6987 - val_accuracy: 0.8000\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 507us/sample - loss: 0.6453 - accuracy: 0.8167 - val_loss: 0.6967 - val_accuracy: 0.8000\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 416us/sample - loss: 0.6433 - accuracy: 0.8167 - val_loss: 0.6949 - val_accuracy: 0.8000\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 469us/sample - loss: 0.6413 - accuracy: 0.8167 - val_loss: 0.6927 - val_accuracy: 0.8000\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 718us/sample - loss: 0.6394 - accuracy: 0.8167 - val_loss: 0.6905 - val_accuracy: 0.8000\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 479us/sample - loss: 0.6374 - accuracy: 0.8250 - val_loss: 0.6880 - val_accuracy: 0.8000\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.6356 - accuracy: 0.8250 - val_loss: 0.6856 - val_accuracy: 0.8333\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 521us/sample - loss: 0.6334 - accuracy: 0.8250 - val_loss: 0.6837 - val_accuracy: 0.8333\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 731us/sample - loss: 0.6316 - accuracy: 0.8250 - val_loss: 0.6820 - val_accuracy: 0.8000\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 513us/sample - loss: 0.6296 - accuracy: 0.8250 - val_loss: 0.6800 - val_accuracy: 0.8333\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 804us/sample - loss: 0.6277 - accuracy: 0.8250 - val_loss: 0.6781 - val_accuracy: 0.8333\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 525us/sample - loss: 0.6258 - accuracy: 0.8250 - val_loss: 0.6763 - val_accuracy: 0.8333\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 484us/sample - loss: 0.6240 - accuracy: 0.8250 - val_loss: 0.6745 - val_accuracy: 0.8333\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 704us/sample - loss: 0.6221 - accuracy: 0.8250 - val_loss: 0.6726 - val_accuracy: 0.8333\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 431us/sample - loss: 0.6203 - accuracy: 0.8250 - val_loss: 0.6708 - val_accuracy: 0.8333\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 0.6184 - accuracy: 0.8333 - val_loss: 0.6688 - val_accuracy: 0.8333\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 695us/sample - loss: 0.6166 - accuracy: 0.8333 - val_loss: 0.6670 - val_accuracy: 0.8333\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 958us/sample - loss: 0.6148 - accuracy: 0.8333 - val_loss: 0.6651 - val_accuracy: 0.8333\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 933us/sample - loss: 0.6130 - accuracy: 0.8333 - val_loss: 0.6632 - val_accuracy: 0.8333\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 613us/sample - loss: 0.6112 - accuracy: 0.8333 - val_loss: 0.6609 - val_accuracy: 0.8333\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 568us/sample - loss: 0.6094 - accuracy: 0.8333 - val_loss: 0.6589 - val_accuracy: 0.8333\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 787us/sample - loss: 0.6076 - accuracy: 0.8333 - val_loss: 0.6571 - val_accuracy: 0.8333\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 840us/sample - loss: 0.6058 - accuracy: 0.8333 - val_loss: 0.6554 - val_accuracy: 0.8333\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 756us/sample - loss: 0.6042 - accuracy: 0.8333 - val_loss: 0.6539 - val_accuracy: 0.8333\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 534us/sample - loss: 0.6024 - accuracy: 0.8333 - val_loss: 0.6522 - val_accuracy: 0.8333\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 852us/sample - loss: 0.6006 - accuracy: 0.8333 - val_loss: 0.6503 - val_accuracy: 0.8333\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 844us/sample - loss: 0.5990 - accuracy: 0.8333 - val_loss: 0.6484 - val_accuracy: 0.8333\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 517us/sample - loss: 0.5973 - accuracy: 0.8333 - val_loss: 0.6467 - val_accuracy: 0.8667\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 529us/sample - loss: 0.5956 - accuracy: 0.8333 - val_loss: 0.6448 - val_accuracy: 0.8667\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 422us/sample - loss: 0.5939 - accuracy: 0.8333 - val_loss: 0.6431 - val_accuracy: 0.8667\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 767us/sample - loss: 0.5923 - accuracy: 0.8333 - val_loss: 0.6416 - val_accuracy: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 446us/sample - loss: 0.5907 - accuracy: 0.8333 - val_loss: 0.6398 - val_accuracy: 0.8667\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 566us/sample - loss: 0.5890 - accuracy: 0.8333 - val_loss: 0.6383 - val_accuracy: 0.8667\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 785us/sample - loss: 0.5874 - accuracy: 0.8333 - val_loss: 0.6368 - val_accuracy: 0.8667\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 589us/sample - loss: 0.5858 - accuracy: 0.8333 - val_loss: 0.6354 - val_accuracy: 0.8667\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 702us/sample - loss: 0.5842 - accuracy: 0.8333 - val_loss: 0.6338 - val_accuracy: 0.8667\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 493us/sample - loss: 0.5828 - accuracy: 0.8333 - val_loss: 0.6325 - val_accuracy: 0.8667\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.78 - 0s 869us/sample - loss: 0.5810 - accuracy: 0.8333 - val_loss: 0.6308 - val_accuracy: 0.8667\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 885us/sample - loss: 0.5794 - accuracy: 0.8333 - val_loss: 0.6290 - val_accuracy: 0.8667\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 510us/sample - loss: 0.5778 - accuracy: 0.8333 - val_loss: 0.6274 - val_accuracy: 0.8667\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 480us/sample - loss: 0.5765 - accuracy: 0.8333 - val_loss: 0.6256 - val_accuracy: 0.9000\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 475us/sample - loss: 0.5748 - accuracy: 0.8333 - val_loss: 0.6240 - val_accuracy: 0.9000\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 456us/sample - loss: 0.5732 - accuracy: 0.8333 - val_loss: 0.6225 - val_accuracy: 0.9000\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 403us/sample - loss: 0.5716 - accuracy: 0.8333 - val_loss: 0.6210 - val_accuracy: 0.8667\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 462us/sample - loss: 0.5701 - accuracy: 0.8333 - val_loss: 0.6195 - val_accuracy: 0.8667\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.5686 - accuracy: 0.8333 - val_loss: 0.6179 - val_accuracy: 0.8667\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 734us/sample - loss: 0.5672 - accuracy: 0.8333 - val_loss: 0.6166 - val_accuracy: 0.9000\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 536us/sample - loss: 0.5656 - accuracy: 0.8333 - val_loss: 0.6149 - val_accuracy: 0.8667\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.5642 - accuracy: 0.8333 - val_loss: 0.6133 - val_accuracy: 0.8667\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 772us/sample - loss: 0.5628 - accuracy: 0.8333 - val_loss: 0.6117 - val_accuracy: 0.9000\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 653us/sample - loss: 0.5612 - accuracy: 0.8333 - val_loss: 0.6105 - val_accuracy: 0.8667\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 881us/sample - loss: 0.5599 - accuracy: 0.8333 - val_loss: 0.6093 - val_accuracy: 0.8667\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 826us/sample - loss: 0.5583 - accuracy: 0.8333 - val_loss: 0.6077 - val_accuracy: 0.8667\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 506us/sample - loss: 0.5568 - accuracy: 0.8333 - val_loss: 0.6062 - val_accuracy: 0.8667\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 460us/sample - loss: 0.5554 - accuracy: 0.8333 - val_loss: 0.6047 - val_accuracy: 0.8667\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 522us/sample - loss: 0.5540 - accuracy: 0.8333 - val_loss: 0.6030 - val_accuracy: 0.9000\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 636us/sample - loss: 0.5526 - accuracy: 0.8333 - val_loss: 0.6015 - val_accuracy: 0.9000\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 777us/sample - loss: 0.5510 - accuracy: 0.8333 - val_loss: 0.6001 - val_accuracy: 0.9000\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 535us/sample - loss: 0.5496 - accuracy: 0.8333 - val_loss: 0.5986 - val_accuracy: 0.9000\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 508us/sample - loss: 0.5484 - accuracy: 0.8333 - val_loss: 0.5969 - val_accuracy: 0.9000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 538us/sample - loss: 0.5469 - accuracy: 0.8333 - val_loss: 0.5954 - val_accuracy: 0.9000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 743us/sample - loss: 0.5455 - accuracy: 0.8333 - val_loss: 0.5942 - val_accuracy: 0.9000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 605us/sample - loss: 0.5440 - accuracy: 0.8333 - val_loss: 0.5928 - val_accuracy: 0.9000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 864us/sample - loss: 0.5427 - accuracy: 0.8333 - val_loss: 0.5915 - val_accuracy: 0.9000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 870us/sample - loss: 0.5414 - accuracy: 0.8333 - val_loss: 0.5902 - val_accuracy: 0.9000\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 617us/sample - loss: 0.5401 - accuracy: 0.8333 - val_loss: 0.5892 - val_accuracy: 0.9000\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 492us/sample - loss: 0.5388 - accuracy: 0.8333 - val_loss: 0.5877 - val_accuracy: 0.9000\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 456us/sample - loss: 0.5373 - accuracy: 0.8333 - val_loss: 0.5861 - val_accuracy: 0.9000\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 650us/sample - loss: 0.5360 - accuracy: 0.8333 - val_loss: 0.5847 - val_accuracy: 0.9000\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 447us/sample - loss: 0.5346 - accuracy: 0.8333 - val_loss: 0.5835 - val_accuracy: 0.9000\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 497us/sample - loss: 0.5335 - accuracy: 0.8333 - val_loss: 0.5820 - val_accuracy: 0.9000\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 445us/sample - loss: 0.5321 - accuracy: 0.8333 - val_loss: 0.5806 - val_accuracy: 0.9000\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 490us/sample - loss: 0.5308 - accuracy: 0.8333 - val_loss: 0.5796 - val_accuracy: 0.9000\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 862us/sample - loss: 0.5295 - accuracy: 0.8333 - val_loss: 0.5782 - val_accuracy: 0.9000\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 735us/sample - loss: 0.5282 - accuracy: 0.8333 - val_loss: 0.5770 - val_accuracy: 0.9000\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 513us/sample - loss: 0.5270 - accuracy: 0.8333 - val_loss: 0.5758 - val_accuracy: 0.9000\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 841us/sample - loss: 0.5256 - accuracy: 0.8333 - val_loss: 0.5744 - val_accuracy: 0.9000\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 709us/sample - loss: 0.5244 - accuracy: 0.8333 - val_loss: 0.5731 - val_accuracy: 0.9000\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 399us/sample - loss: 0.5231 - accuracy: 0.8333 - val_loss: 0.5718 - val_accuracy: 0.9000\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 711us/sample - loss: 0.5221 - accuracy: 0.8333 - val_loss: 0.5703 - val_accuracy: 0.9000\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.5206 - accuracy: 0.8333 - val_loss: 0.5691 - val_accuracy: 0.9000\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 782us/sample - loss: 0.5195 - accuracy: 0.8333 - val_loss: 0.5678 - val_accuracy: 0.9000\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 533us/sample - loss: 0.5184 - accuracy: 0.8333 - val_loss: 0.5668 - val_accuracy: 0.9000\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 449us/sample - loss: 0.5170 - accuracy: 0.8333 - val_loss: 0.5656 - val_accuracy: 0.9000\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 489us/sample - loss: 0.5158 - accuracy: 0.8333 - val_loss: 0.5642 - val_accuracy: 0.9000\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5217 - accuracy: 0.81 - 0s 454us/sample - loss: 0.5146 - accuracy: 0.8333 - val_loss: 0.5631 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 447us/sample - loss: 0.5134 - accuracy: 0.8333 - val_loss: 0.5620 - val_accuracy: 0.9000\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 445us/sample - loss: 0.5122 - accuracy: 0.8333 - val_loss: 0.5607 - val_accuracy: 0.9000\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 487us/sample - loss: 0.5110 - accuracy: 0.8333 - val_loss: 0.5593 - val_accuracy: 0.9000\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 799us/sample - loss: 0.5099 - accuracy: 0.8333 - val_loss: 0.5579 - val_accuracy: 0.9000\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 930us/sample - loss: 0.5086 - accuracy: 0.8333 - val_loss: 0.5568 - val_accuracy: 0.9000\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 587us/sample - loss: 0.5076 - accuracy: 0.8333 - val_loss: 0.5557 - val_accuracy: 0.9000\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 835us/sample - loss: 0.5065 - accuracy: 0.8333 - val_loss: 0.5542 - val_accuracy: 0.9000\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 538us/sample - loss: 0.5051 - accuracy: 0.8333 - val_loss: 0.5532 - val_accuracy: 0.9000\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 469us/sample - loss: 0.5040 - accuracy: 0.8417 - val_loss: 0.5520 - val_accuracy: 0.9000\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 458us/sample - loss: 0.5028 - accuracy: 0.8333 - val_loss: 0.5508 - val_accuracy: 0.9000\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 514us/sample - loss: 0.5017 - accuracy: 0.8333 - val_loss: 0.5498 - val_accuracy: 0.9000\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 698us/sample - loss: 0.5005 - accuracy: 0.8333 - val_loss: 0.5488 - val_accuracy: 0.9000\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 555us/sample - loss: 0.4998 - accuracy: 0.8333 - val_loss: 0.5481 - val_accuracy: 0.9000\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 488us/sample - loss: 0.4983 - accuracy: 0.8333 - val_loss: 0.5467 - val_accuracy: 0.9000\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 681us/sample - loss: 0.4973 - accuracy: 0.8333 - val_loss: 0.5453 - val_accuracy: 0.9000\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 584us/sample - loss: 0.4961 - accuracy: 0.8333 - val_loss: 0.5441 - val_accuracy: 0.9000\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4851 - accuracy: 0.81 - 0s 844us/sample - loss: 0.4949 - accuracy: 0.8417 - val_loss: 0.5429 - val_accuracy: 0.9000\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 706us/sample - loss: 0.4939 - accuracy: 0.8333 - val_loss: 0.5419 - val_accuracy: 0.9000\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 500us/sample - loss: 0.4927 - accuracy: 0.8417 - val_loss: 0.5408 - val_accuracy: 0.9000\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 778us/sample - loss: 0.4917 - accuracy: 0.8417 - val_loss: 0.5396 - val_accuracy: 0.9000\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 759us/sample - loss: 0.4906 - accuracy: 0.8417 - val_loss: 0.5385 - val_accuracy: 0.9000\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 564us/sample - loss: 0.4895 - accuracy: 0.8417 - val_loss: 0.5374 - val_accuracy: 0.9000\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 538us/sample - loss: 0.4884 - accuracy: 0.8417 - val_loss: 0.5364 - val_accuracy: 0.9000\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 537us/sample - loss: 0.4874 - accuracy: 0.8417 - val_loss: 0.5354 - val_accuracy: 0.9000\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 723us/sample - loss: 0.4863 - accuracy: 0.8417 - val_loss: 0.5342 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fbe583e508>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_X_train,y=y_train,epochs=300,validation_data=(scaler_X_test,y_test),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.183851</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.225993</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.180459</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.222851</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.177767</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.219367</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.174800</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.215927</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.171811</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.212813</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.490586</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.538507</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.489487</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.537400</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.488441</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.536397</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.487403</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.535396</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.486276</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.534210</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    1.183851  0.350000  1.225993      0.266667\n",
       "1    1.180459  0.350000  1.222851      0.266667\n",
       "2    1.177767  0.350000  1.219367      0.266667\n",
       "3    1.174800  0.350000  1.215927      0.266667\n",
       "4    1.171811  0.350000  1.212813      0.266667\n",
       "..        ...       ...       ...           ...\n",
       "295  0.490586  0.841667  0.538507      0.900000\n",
       "296  0.489487  0.841667  0.537400      0.900000\n",
       "297  0.488441  0.841667  0.536397      0.900000\n",
       "298  0.487403  0.841667  0.535396      0.900000\n",
       "299  0.486276  0.841667  0.534210      0.900000\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fbe736a148>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fbe974aac8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4855556885401408, 0.84166664]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaled_X_train,y=y_train,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5342098474502563, 0.9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaler_X_test,y=y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are ready for your model for deployment you can deploy it now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before deploying we want to make sure that we use all our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hence we will train on al our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=len(metrics) #we run for same no of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=4,activation='relu',input_shape=[4,]))\n",
    "#sinc there are only 4 feature and we are not worry about batches hence input shape is [4,]\n",
    "model.add(Dense(units=3,activation='softmax'))# output belong to three class\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])#in metrics we specify that we will keep\n",
    "#trac of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/300\n",
      "150/150 [==============================] - 1s 6ms/sample - loss: 1.1321 - accuracy: 0.3333\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 1.1202 - accuracy: 0.3333\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 1.1096 - accuracy: 0.3333\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 1.0997 - accuracy: 0.3333\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 1.0895 - accuracy: 0.3333\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 1.0800 - accuracy: 0.3333\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 1.0711 - accuracy: 0.3400\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 1.0631 - accuracy: 0.3600\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 1.0547 - accuracy: 0.3800\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 1.0479 - accuracy: 0.3933\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 1.0398 - accuracy: 0.4000\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 1.0327 - accuracy: 0.4333\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 1.0260 - accuracy: 0.4400\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 1.0195 - accuracy: 0.4800\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0131 - accuracy: 0.5133\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 1.0068 - accuracy: 0.5400\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 1.0008 - accuracy: 0.5533\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.9948 - accuracy: 0.5600\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.9891 - accuracy: 0.5733\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.9836 - accuracy: 0.5800\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.9781 - accuracy: 0.5800\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.9726 - accuracy: 0.5867\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.9671 - accuracy: 0.6000\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.9620 - accuracy: 0.6133\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.9565 - accuracy: 0.6200\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.9515 - accuracy: 0.6200\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.9462 - accuracy: 0.6200\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.9409 - accuracy: 0.6200\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.9356 - accuracy: 0.6200\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.9306 - accuracy: 0.6200\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.9253 - accuracy: 0.6267\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.9201 - accuracy: 0.6333\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.9150 - accuracy: 0.6400\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.9097 - accuracy: 0.6400\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.9044 - accuracy: 0.6400\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.8993 - accuracy: 0.6533\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.8938 - accuracy: 0.6600\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.8886 - accuracy: 0.6667\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.8833 - accuracy: 0.6733\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.8780 - accuracy: 0.6733\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.8726 - accuracy: 0.6733\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.8672 - accuracy: 0.7000\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.8619 - accuracy: 0.7000\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.8565 - accuracy: 0.7200\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.8511 - accuracy: 0.7200\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.8209 - accuracy: 0.71 - 0s 104us/sample - loss: 0.8457 - accuracy: 0.7200\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.8402 - accuracy: 0.7200\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.8347 - accuracy: 0.7200\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.8293 - accuracy: 0.7200\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.8238 - accuracy: 0.7333\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.8183 - accuracy: 0.7400\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.8129 - accuracy: 0.7400\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.8075 - accuracy: 0.7400\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8019 - accuracy: 0.7533\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.7965 - accuracy: 0.7533\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.7911 - accuracy: 0.7400\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.7856 - accuracy: 0.7400\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.7802 - accuracy: 0.7400\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.7749 - accuracy: 0.7400\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.7695 - accuracy: 0.7467\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.7641 - accuracy: 0.7400\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.7588 - accuracy: 0.7400\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.7535 - accuracy: 0.7400\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.7482 - accuracy: 0.7400\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.7431 - accuracy: 0.7400\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.7379 - accuracy: 0.7333\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.7328 - accuracy: 0.7333\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.7278 - accuracy: 0.7333\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.7226 - accuracy: 0.7333\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.7177 - accuracy: 0.7333\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.7128 - accuracy: 0.7333\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.7079 - accuracy: 0.7333\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.7031 - accuracy: 0.7333\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.6984 - accuracy: 0.7333\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.6936 - accuracy: 0.7267\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.6889 - accuracy: 0.7267\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.6842 - accuracy: 0.7267\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 107us/sample - loss: 0.6797 - accuracy: 0.7200\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.6753 - accuracy: 0.7200\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.6709 - accuracy: 0.7200\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.6665 - accuracy: 0.7200\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.6622 - accuracy: 0.7200\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.6579 - accuracy: 0.7200\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.6538 - accuracy: 0.7200\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.6496 - accuracy: 0.7200\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.6456 - accuracy: 0.7200\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.6415 - accuracy: 0.7200\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.6376 - accuracy: 0.7200\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.6338 - accuracy: 0.7200\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.6298 - accuracy: 0.7200\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.6261 - accuracy: 0.7200\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.6224 - accuracy: 0.7200\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.6187 - accuracy: 0.7200\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.6151 - accuracy: 0.7200\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.6115 - accuracy: 0.7200\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.6081 - accuracy: 0.7267\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.6047 - accuracy: 0.7333\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.6011 - accuracy: 0.7333\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.5978 - accuracy: 0.7333\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.5946 - accuracy: 0.7333\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.5913 - accuracy: 0.7333\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.5881 - accuracy: 0.7467\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5850 - accuracy: 0.7467\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.5819 - accuracy: 0.7533\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.5789 - accuracy: 0.7600\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.5758 - accuracy: 0.7667\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5730 - accuracy: 0.7667\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.5701 - accuracy: 0.7667\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5673 - accuracy: 0.7600\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.5645 - accuracy: 0.7600\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.5618 - accuracy: 0.7667\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.5591 - accuracy: 0.7667\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.5566 - accuracy: 0.7667\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5539 - accuracy: 0.7667\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.5513 - accuracy: 0.7867\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.5488 - accuracy: 0.7867\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5463 - accuracy: 0.7867\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.5439 - accuracy: 0.7867\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.5415 - accuracy: 0.7933\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.5393 - accuracy: 0.7933\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.5369 - accuracy: 0.7933\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.5346 - accuracy: 0.7933\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5323 - accuracy: 0.7933\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.5302 - accuracy: 0.7933\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.5280 - accuracy: 0.7933\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.5257 - accuracy: 0.8000\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.5237 - accuracy: 0.8000\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.5215 - accuracy: 0.8000\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.5194 - accuracy: 0.8067\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.5174 - accuracy: 0.8133\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.5154 - accuracy: 0.8133\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 162us/sample - loss: 0.5134 - accuracy: 0.8133\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.5114 - accuracy: 0.8133\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5096 - accuracy: 0.8133\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.5075 - accuracy: 0.8200\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.5057 - accuracy: 0.8133\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.84 - 0s 99us/sample - loss: 0.5039 - accuracy: 0.8133\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.5021 - accuracy: 0.8133\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.5002 - accuracy: 0.8200\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.4984 - accuracy: 0.8200\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.4967 - accuracy: 0.8267\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4949 - accuracy: 0.8267\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4932 - accuracy: 0.8267\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4915 - accuracy: 0.8267\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4899 - accuracy: 0.8333\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.4882 - accuracy: 0.8333\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4866 - accuracy: 0.8333\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.4850 - accuracy: 0.8333\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.4834 - accuracy: 0.8333\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.4820 - accuracy: 0.8333\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4801 - accuracy: 0.8333\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.4786 - accuracy: 0.8333\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.4770 - accuracy: 0.8333\n",
      "Epoch 154/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4755 - accuracy: 0.8333\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4739 - accuracy: 0.8333\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.4723 - accuracy: 0.8400\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.4709 - accuracy: 0.8400\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.4693 - accuracy: 0.8467\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4678 - accuracy: 0.8467\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.4663 - accuracy: 0.8467\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.4648 - accuracy: 0.8533\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.4634 - accuracy: 0.8533\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.4619 - accuracy: 0.8533\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4606 - accuracy: 0.8533\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.4591 - accuracy: 0.8600\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.4577 - accuracy: 0.8600\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.4562 - accuracy: 0.8533\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.4548 - accuracy: 0.8533\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.4534 - accuracy: 0.8600\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4521 - accuracy: 0.8600\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4508 - accuracy: 0.8600\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.4495 - accuracy: 0.8600\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.4481 - accuracy: 0.8600\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4467 - accuracy: 0.8600\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4455 - accuracy: 0.8600\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4442 - accuracy: 0.8600\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.81 - 0s 135us/sample - loss: 0.4429 - accuracy: 0.8600\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4416 - accuracy: 0.8600\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.4403 - accuracy: 0.8600\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.4390 - accuracy: 0.8600\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4378 - accuracy: 0.8600\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4365 - accuracy: 0.8600\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.4353 - accuracy: 0.8600\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.4342 - accuracy: 0.8600\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.4328 - accuracy: 0.8600\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.4316 - accuracy: 0.8600\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.4305 - accuracy: 0.8667\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.4292 - accuracy: 0.8667\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.4282 - accuracy: 0.8667\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 146us/sample - loss: 0.4270 - accuracy: 0.8667\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4258 - accuracy: 0.8667\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4246 - accuracy: 0.8667\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4236 - accuracy: 0.8667\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4223 - accuracy: 0.8667\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4212 - accuracy: 0.8733\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.4200 - accuracy: 0.8733\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.4189 - accuracy: 0.8733\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4177 - accuracy: 0.8733\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4166 - accuracy: 0.8867\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4156 - accuracy: 0.8867\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4145 - accuracy: 0.8933\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.4133 - accuracy: 0.8933\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4122 - accuracy: 0.8933\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.4111 - accuracy: 0.8933\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.4100 - accuracy: 0.8933\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4090 - accuracy: 0.8933\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4080 - accuracy: 0.8933\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4068 - accuracy: 0.8933\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4059 - accuracy: 0.8933\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4047 - accuracy: 0.8933\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.4037 - accuracy: 0.8933\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.4027 - accuracy: 0.8933\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.4016 - accuracy: 0.8933\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.4007 - accuracy: 0.8933\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.3995 - accuracy: 0.8933\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 245us/sample - loss: 0.3985 - accuracy: 0.8933\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3977 - accuracy: 0.9000\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.3965 - accuracy: 0.9000\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3954 - accuracy: 0.9000\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3944 - accuracy: 0.9067\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3933 - accuracy: 0.9067\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3924 - accuracy: 0.9067\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.3914 - accuracy: 0.9067\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3903 - accuracy: 0.9067\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.3893 - accuracy: 0.9000\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3883 - accuracy: 0.9000\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3873 - accuracy: 0.8933\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3864 - accuracy: 0.9000\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3854 - accuracy: 0.9000\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3843 - accuracy: 0.9000\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.3834 - accuracy: 0.9000\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3823 - accuracy: 0.9000\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3813 - accuracy: 0.9000\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3803 - accuracy: 0.9000\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3793 - accuracy: 0.9000\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3785 - accuracy: 0.9000\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.90 - 0s 124us/sample - loss: 0.3774 - accuracy: 0.9067\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3763 - accuracy: 0.9067\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3753 - accuracy: 0.9067\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3744 - accuracy: 0.9000\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.3733 - accuracy: 0.9000\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3723 - accuracy: 0.9000\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.3714 - accuracy: 0.9000\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3704 - accuracy: 0.9000\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3694 - accuracy: 0.9000\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.93 - 0s 111us/sample - loss: 0.3684 - accuracy: 0.9000\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3674 - accuracy: 0.9000\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3664 - accuracy: 0.9000\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3655 - accuracy: 0.9000\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3644 - accuracy: 0.9000\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3636 - accuracy: 0.9000\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.3625 - accuracy: 0.9000\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3616 - accuracy: 0.9000\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3606 - accuracy: 0.9067\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3596 - accuracy: 0.9067\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3585 - accuracy: 0.9067\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3576 - accuracy: 0.9067\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3568 - accuracy: 0.9067\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.3557 - accuracy: 0.9000\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3549 - accuracy: 0.9000\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3539 - accuracy: 0.8933\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3529 - accuracy: 0.8933\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3520 - accuracy: 0.9000\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3510 - accuracy: 0.9000\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3500 - accuracy: 0.9067\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3492 - accuracy: 0.9067\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3484 - accuracy: 0.9067\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3472 - accuracy: 0.9067\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3463 - accuracy: 0.9067\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3456 - accuracy: 0.9133\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3448 - accuracy: 0.9133\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3436 - accuracy: 0.9133\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3427 - accuracy: 0.9067\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3417 - accuracy: 0.9067\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3410 - accuracy: 0.9000\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3400 - accuracy: 0.9000\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3390 - accuracy: 0.9000\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.3383 - accuracy: 0.9000\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.3373 - accuracy: 0.9000\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3364 - accuracy: 0.9000\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3355 - accuracy: 0.9000\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3346 - accuracy: 0.9067\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3340 - accuracy: 0.9133\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3329 - accuracy: 0.9133\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3320 - accuracy: 0.9133\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3311 - accuracy: 0.9133\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3302 - accuracy: 0.9133\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3294 - accuracy: 0.9000\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.3284 - accuracy: 0.9067\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3276 - accuracy: 0.9067\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3267 - accuracy: 0.9133\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3259 - accuracy: 0.9133\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3250 - accuracy: 0.9133\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3241 - accuracy: 0.9133\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3234 - accuracy: 0.9267\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3224 - accuracy: 0.9267\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.3215 - accuracy: 0.9200\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3207 - accuracy: 0.9200\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.3198 - accuracy: 0.9200\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3189 - accuracy: 0.9200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fbe99d82c8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X,y,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_iris_model_self.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to save scaler itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data have to save scaler itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib # it is built in python hence we don't need to download anything to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,\"iris_scaler.pkl\")# we need to pass as value object and filename in which to save it\n",
    "#we need to save it in pikle file ie .pkl extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save our model in .h5 model and scaler in .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    " from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_model=load_model('final_iris_model_self.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_scaler=joblib.load(\"iris_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on this model we can predict something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in next lecture we are going to how to take example json file flower_model and flower_scaler and return back example json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we take dat process data and return data in prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jason is used for this data data exchange and jason stand for JAVSCRIPT OBJECT NOTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#json is similar to python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exaple data point will actually look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example={\n",
    "    \n",
    "                    \"sepal_length\":5.1,\n",
    "                    \"sepal_width\":3.5,\n",
    "                    \"petal_length\":1.4,\n",
    "                    \"petal_width\":0.2,\n",
    "                        \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flower_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_ # the order is same in which we have transformed our y in on binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prediction(model,scaler,sample_json):\n",
    "    s_len=sample_json['sepal_length']\n",
    "    s_wid=sample_json['sepal_width']\n",
    "    p_len=sample_json['petal_length']\n",
    "    p_wid=sample_json['petal_width']\n",
    "    classes=np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    flower=[[s_len,s_wid,p_len,p_wid]]   #model expect in this format hence we pass this\n",
    "    flower=scaler.transform(flower)     # scaling flower based on scaler passed\n",
    "    class_ind=model.predict_classes(flower)[0]\n",
    "    # model return indices of flower as 0 1 or 2 accordingly in order as in classes\n",
    "    #model return array\n",
    "    return classes[class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_prediction(flower_model,flower_scaler,flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatever code we need for deployment we will take in single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "flower_model=load_model('final_iris_model_self.h5')\n",
    "flower_scaler=joblib.load(\"iris_scaler.pkl\")\n",
    "def return_prediction(model,scaler,sample_json):\n",
    "    s_len=sample_json['sepal_length']\n",
    "    s_wid=sample_json['sepal_width']\n",
    "    p_len=sample_json['petal_length']\n",
    "    p_wid=sample_json['petal_width']\n",
    "    classes=np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    flower=[[s_len,s_wid,p_len,p_wid]]   #model expect in this format hence we pass this\n",
    "    flower=scaler.transform(flower)     # scaling flower based on scaler passed\n",
    "    class_ind=model.predict_classes(flower)[0]\n",
    "    # model return indices of flower as 0 1 or 2 accordingly in order as in classes\n",
    "    #model return array\n",
    "    return classes[class_ind]\n",
    "return_prediction(flower_model,flower_scaler,flower_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only this much code we will need to launch our flask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will deploy our model using prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flask is python based web application framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flask uses backend to handle backend of web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can later connect flask to frontend component such as html and css"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will integrate tensorflow model and prediction function inside flask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to interact with falask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are two way to do this\n",
    "    # API REQUEST\n",
    "    #postman  is a program which can send json information to flask application interactiong with it\n",
    "    #and it get back result from model\n",
    "    \n",
    "    #we can do it with python as well\n",
    "        #we can use request library built in python send that over jason information and get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after that we connect flask with html form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user will visit a application they will provide number for each of model , model runs through prediction \n",
    "#and return on new page prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
